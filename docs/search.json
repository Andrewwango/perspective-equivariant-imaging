[
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "",
    "text": "Webpage | Notebook\nAuthors: Andrew Wang, Mike Davies, School of Engineering, University of Edinburgh\nAbstract: Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone. We propose (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature.\nCitation"
  },
  {
    "objectID": "demo.html#results",
    "href": "demo.html#results",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "1. Results",
    "text": "1. Results\n\n\nimport deepinv as dinv\nfrom perspective_ei import *\ndevice = get_device()"
  },
  {
    "objectID": "demo.html#background",
    "href": "demo.html#background",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "2. Background",
    "text": "2. Background\n\n2.1 Multispectral pansharpening\nThe pansharpening inverse problem seeks to recover high-resolution multispectral (HRMS) images \\(x\\) from low-resolution multispectral (LRMS) \\(\\mathbf{y}_\\text{MS}\\) and high-resolution panchromatic (single-band) \\(\\mathbf{y}_\\text{pan}\\) images. The forward operator is:\n\\[\\left\\{\\mathbf{y}_\\text{MS},\\mathbf{y}_\\text{PAN}\\right\\}\\sim\\mathcal{P}\\left(\\mathbf{A}_\\text{PS}\\mathbf{x}\\right)=\\left\\{\\mathcal{P}\\left(\\mathbf{A}_\\text{SR}\\mathbf{x}\\right),\\mathcal{P}\\left(\\mathbf{R}_\\text{PAN}\\mathbf{x}\\right)\\right\\}\\]\nwhere \\(\\mathbf{R}_\\text{PAN}\\) is the pan channel’s SRF, \\(\\mathbf{A}_\\text{SR}\\mathbf{x}=(\\mathbf{k} * \\mathbf{x}) \\downarrow_j\\) is the \\(j\\times j\\)-factor downsampling operator with anti-aliasing kernel \\(\\mathbf{k}\\), and \\(\\mathcal{P}\\) is the Poisson noise operator. We simulate a tiny dataset of WorldView-2 tiles taken from SpaceNet-4:\n\ndataset_name = \"spacenet\"\nnoise_gain = 0.\nimg_shape = (4, 1024, 1024)\nratio = 4\n\n\nphysics = Pansharpen(img_shape, factor=ratio, device=device)\n\nif noise_gain &gt; 0.0:\n    physics.noise_model = dinv.physics.PoissonNoise(gain=noise_gain, clip_positive=True) \n\n\ntrain_dataloader, test_dataloader = make_dataloaders(dataset_name, physics, device=device)\n\n\nx, y = next(iter(test_dataloader))\nplot_multispectral(x, y)\n\n\n\n\n\n\n2.2 Projective transformations\nCamera-based imaging systems move and rotate freely in the world. In satellite imaging, scenes are generally imaged off-nadir, i.e. the focal line does not pass through the Earth point located vertically below, resulting in a perspective distortion.\nOur proposed perspective-EI uses the natural belief that unknown image sets are invariant to changes in perspective to solve these inverse problems without ground truth (GT), by considering the group of non-linear projective transformations \\(\\mathbf{T}_g\\). Note that we do not have access to the GT images \\(x\\), we just show these for demonstration:\n\ntransform = Homography(\n    n_trans = 1, \n    theta_max = 5, #degrees \n    theta_z_max = 0, \n    shift_max = 0, \n    skew_max = 0,\n    zoom_factor_min = 1,\n    x_stretch_factor_min = 1, \n    y_stretch_factor_min = 1, \n    device=device\n    )\n\nplot_multispectral([x, transform(x), transform(x), transform(x)])\n\n\n\n\n\n\n2.3 Perspective-equivariant imaging\nThe EI framework learns to invert from measurements \\(y\\) alone:\n\nOur proposed loss function adds perspective-EI onto a spectral and structural measurement consistency (MC) loss:\n\\[\\mathcal{L}_\\text{unsup}(\\theta;\\mathbf{y},g)=\\lVert\\mathbf{A}f_\\theta(\\mathbf{y})-\\mathbf{y}_\\text{MS}\\rVert_2^2+\\lVert \\mathbf{R}_\\text{pan}f_\\theta(\\mathbf{y})-\\mathbf{y}_\\text{pan}\\rVert_\\text{TV}+\\mathcal{L}_\\text{EI}(\\theta;\\mathbf{y},g)\\]\nIn the noisy scenario, both the spectral and structural MC losses are replaced by SURE losses.\n\nif noise_gain &gt; 0:\n    loss_spectral   = SurePoissonSpectralLoss(gain=noise_gain, tau=1e-2) #tau from (Chen et al. 2022: Robust Equivariant Imaging, Appendix C.3)\n    loss_structural = SurePoissonStructuralLoss(gain=noise_gain, tau=1e-2, srf_from=\"average\")\nelse:\n    loss_spectral   = dinv.loss.MCLoss(metric=LRMS_MSELoss(ratio))\n    loss_structural = TVStructuralLoss(srf_from=\"average\")\n\nlosses = [loss_spectral, loss_structural, dinv.loss.EILoss(transform, metric=HRMS_MSELoss(), weight=1.)]"
  },
  {
    "objectID": "demo.html#training",
    "href": "demo.html#training",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "3. Training",
    "text": "3. Training\nWe train with the PanNet neural network, but any NN model can be used since our method is independent of the choice of NN. We train our models using deepinv which is built on PyTorch.\n\nmodel = PanNet(\n    backbone_net=ResNet(hidden_channels=32, num_blocks=4), \n    hrms_shape=img_shape, \n    scale_factor=ratio,\n    highpass_kernel_size=51,\n    device=device,\n).to(device)\n\n\noptimizer, scheduler = make_optimizer_scheduler(model, lr_init=1e-3)\n\nmodel = dinv.train(\n    epochs=5,\n    model=model,\n    losses=losses,\n    physics=physics,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    scheduler=scheduler,\n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n    ckp_interval=99,\n    img_interval=99,\n    freq_plot=99,\n)\n\n\n3.1 Evaluation metrics\nWe report QNR (no reference) ERGAS, and PSNR. See Meng et al. benchmark for details.\n\nmodel.eval()\n\nx_hat = model(y.to(device), physics).detach().cpu()\n\nqnr = QNR(physics, alpha=1., beta=1.5)(x_hat, y)\nfrom torchmetrics.functional.image import error_relative_global_dimensionless_synthesis\nergas = error_relative_global_dimensionless_synthesis(x_hat, x, ratio=0.25).item()\npsnr = dinv.utils.cal_psnr(x_hat, x)\n\nprint(f\"QNR: {round(qnr, 3)}, PSNR: {round(psnr, 2)}, ERGAS: {round(ergas, 2)}\")\n\nc:\\Users\\s2558406\\Documents\\Repos\\ei-experiments\\venv\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n  _future_warning(\n\n\nQNR: 0.005, PSNR: 6.85, ERGAS: 27.65"
  },
  {
    "objectID": "demo.html#competitors",
    "href": "demo.html#competitors",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "4. Competitors",
    "text": "4. Competitors\n\n# TODO training options for running competitors"
  }
]