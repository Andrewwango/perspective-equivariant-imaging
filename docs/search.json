[
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "",
    "text": "Demo webpage | Demo notebook\nAuthors: Andrew Wang, Mike Davies, School of Engineering, University of Edinburgh\nAbstract: Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone. We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature.\nCitation"
  },
  {
    "objectID": "demo.html#results",
    "href": "demo.html#results",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "1. Results",
    "text": "1. Results\n\n\nimport deepinv as dinv\nfrom perspective_ei import *\ndevice = get_device()"
  },
  {
    "objectID": "demo.html#background",
    "href": "demo.html#background",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "2. Background",
    "text": "2. Background\n\n2.1 Multispectral pansharpening\nThe pansharpening inverse problem seeks to recover high-resolution multispectral (HRMS) images \\(x\\) from low-resolution multispectral (LRMS) \\(\\mathbf{y}_\\text{MS}\\) and high-resolution panchromatic (single-band) \\(\\mathbf{y}_\\text{pan}\\) images. The forward operator is:\n\\[\\left\\{\\mathbf{y}_\\text{MS},\\mathbf{y}_\\text{PAN}\\right\\}\\sim\\mathcal{P}\\left(\\mathbf{A}_\\text{PS}\\mathbf{x}\\right)=\\left\\{\\mathcal{P}\\left(\\mathbf{A}_\\text{SR}\\mathbf{x}\\right),\\mathcal{P}\\left(\\mathbf{R}_\\text{PAN}\\mathbf{x}\\right)\\right\\}\\]\nwhere \\(\\mathbf{R}_\\text{PAN}\\) is the pan channel’s SRF, \\(\\mathbf{A}_\\text{SR}\\mathbf{x}=(\\mathbf{k} * \\mathbf{x}) \\downarrow_j\\) is the \\(j\\times j\\)-factor downsampling operator with anti-aliasing kernel \\(\\mathbf{k}\\), and \\(\\mathcal{P}\\) is the Poisson noise operator. We simulate a tiny dataset of WorldView-2 tiles taken from SpaceNet-4 (see vis):\n\ndataset_name = \"spacenet\"\nnoise_gain = 0.\nimg_shape = (4, 1024, 1024)\nratio = 4\n\n\nphysics = Pansharpen(img_shape, factor=ratio, device=device)\n\nif noise_gain &gt; 0.0:\n    physics.noise_model = dinv.physics.PoissonNoise(gain=noise_gain, clip_positive=True) \n\n\ntrain_dataloader, test_dataloader = make_dataloaders(dataset_name, physics, device=device)\n\n\nx, y = next(iter(test_dataloader))\nx, y = x.to(device), y.to(device)\nplot_multispectral(x, y)\n\n\n\n\n\n\n2.2 Projective transformations\nCamera-based imaging systems move and rotate freely in the world. In satellite imaging, scenes are generally imaged off-nadir, i.e. the focal line does not pass through the Earth point located vertically below, resulting in a perspective distortion.\nOur proposed perspective-EI uses the natural belief that unknown image sets are invariant to changes in perspective to solve these inverse problems without ground truth (GT), by considering the group of non-linear projective transformations \\(\\mathbf{T}_g\\). Note that we do not have access to the GT images \\(x\\), we just show these for demonstration:\n\ntransform = Homography(\n    n_trans = 1, \n    theta_max = 5, #degrees \n    theta_z_max = 0, \n    shift_max = 0, \n    skew_max = 0,\n    zoom_factor_min = 1,\n    x_stretch_factor_min = 1, \n    y_stretch_factor_min = 1, \n    device=device\n    )\n\nplot_multispectral([x, transform(x), transform(x), transform(x)])\n\n\n\n\n\n\n2.3 Perspective-equivariant imaging\nThe EI framework learns to invert from measurements \\(y\\) alone:\n\nOur proposed loss function adds perspective-EI onto a spectral and structural measurement consistency (MC) loss:\n\\[\\mathcal{L}_\\text{unsup}(\\theta;\\mathbf{y},g)=\\lVert\\mathbf{A}f_\\theta(\\mathbf{y})-\\mathbf{y}_\\text{MS}\\rVert_2^2+\\lVert \\mathbf{R}_\\text{pan}f_\\theta(\\mathbf{y})-\\mathbf{y}_\\text{pan}\\rVert_\\text{TV}+\\mathcal{L}_\\text{EI}(\\theta;\\mathbf{y},g)\\]\nIn the noisy scenario, both the spectral and structural MC losses are replaced by SURE losses. We assume a flat \\(\\mathbf{R}_\\text{pan}\\) that simply averages the channels.\n\nif noise_gain &gt; 0:\n    loss_spectral   = SurePoissonSpectralLoss(gain=noise_gain, tau=1e-2) #tau from (Chen et al. 2022: Robust Equivariant Imaging, Appendix C.3)\n    loss_structural = SurePoissonStructuralLoss(gain=noise_gain, tau=1e-2, srf_from=\"average\")\nelse:\n    loss_spectral   = dinv.loss.MCLoss(metric=LRMS_MSELoss(ratio))\n    loss_structural = TVStructuralLoss(srf_from=\"average\")\n\nlosses = [loss_spectral, loss_structural, dinv.loss.EILoss(transform, metric=HRMS_MSELoss(), weight=1.)]"
  },
  {
    "objectID": "demo.html#training",
    "href": "demo.html#training",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "3. Training",
    "text": "3. Training\nWe train with the PanNet neural network, but any NN model can be used since our method is independent of the choice of NN. We train our models using deepinv which is built on PyTorch.\n\nmodel = PanNet(\n    backbone_net=ResNet(hidden_channels=32, num_blocks=4), \n    hrms_shape=img_shape, \n    scale_factor=ratio,\n    highpass_kernel_size=51,\n    device=device,\n).to(device)\n\n\noptimizer, scheduler = make_optimizer_scheduler(model, lr_init=1e-3)\n\nmodel = dinv.train(\n    epochs=10,\n    model=model,\n    losses=losses,\n    physics=physics,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    scheduler=scheduler,\n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n    ckp_interval=99,\n    img_interval=99,\n    freq_plot=99,\n)\n\nThe model has 77124 trainable parameters\n\n\nEpoch 1: 100%|█| 10/10 [00:04&lt;00:00,  2.46it/s, eval_psnr=20.3, loss_mc=0.0103, loss_TVStructural=0.000693, loss_ei=0.00\nEpoch 2: 100%|█| 10/10 [00:03&lt;00:00,  2.55it/s, eval_psnr=19.2, loss_mc=0.00687, loss_TVStructural=0.000447, loss_ei=0.0\nEpoch 3: 100%|█| 10/10 [00:03&lt;00:00,  2.51it/s, eval_psnr=19.5, loss_mc=0.00644, loss_TVStructural=0.000495, loss_ei=0.0\nEpoch 4: 100%|█| 10/10 [00:03&lt;00:00,  2.53it/s, eval_psnr=21.6, loss_mc=0.00677, loss_TVStructural=0.000509, loss_ei=0.0\nEpoch 5: 100%|█| 10/10 [00:03&lt;00:00,  2.53it/s, eval_psnr=21.5, loss_mc=0.00596, loss_TVStructural=0.00043, loss_ei=0.00\nEpoch 6: 100%|█| 10/10 [00:03&lt;00:00,  2.53it/s, eval_psnr=21.2, loss_mc=0.00577, loss_TVStructural=0.000453, loss_ei=0.0\nEpoch 7: 100%|█| 10/10 [00:04&lt;00:00,  2.48it/s, eval_psnr=22.4, loss_mc=0.00519, loss_TVStructural=0.000444, loss_ei=0.0\nEpoch 8: 100%|█| 10/10 [00:04&lt;00:00,  2.50it/s, eval_psnr=22.9, loss_mc=0.00497, loss_TVStructural=0.000475, loss_ei=0.0\nEpoch 9: 100%|█| 10/10 [00:03&lt;00:00,  2.51it/s, eval_psnr=22.2, loss_mc=0.00556, loss_TVStructural=0.000422, loss_ei=0.0\nEpoch 10: 100%|█| 10/10 [00:03&lt;00:00,  2.54it/s, eval_psnr=21.1, loss_mc=0.00503, loss_TVStructural=0.000391, loss_ei=0.\n\n\n\n3.1 Evaluation metrics\nWe report QNR (no reference) ERGAS, and PSNR. See Meng et al. benchmark for details.\n\nmodel.eval()\n\nx_hat = model(y.to(device), physics).detach()\n\nqnr = QNR(physics, alpha=1., beta=1.5)(x_hat, y)\nfrom torchmetrics.functional.image import error_relative_global_dimensionless_synthesis\nergas = error_relative_global_dimensionless_synthesis(x_hat, x, ratio=0.25).item()\npsnr = dinv.utils.cal_psnr(x_hat, x)\n\nprint(f\"QNR: {round(qnr, 3)}, PSNR: {round(psnr, 2)}, ERGAS: {round(ergas, 2)}\")\n\nQNR: 0.776, PSNR: 18.65, ERGAS: 8.34"
  },
  {
    "objectID": "demo.html#competitors",
    "href": "demo.html#competitors",
    "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening",
    "section": "4. Competitors",
    "text": "4. Competitors\nWe compare our pansharpening loss with 4 unsupervised methods from the literature: the losses from GDD, SSQ, Z-PNN and PanGan.\nTo train with them, simply replace losses with one of the following:\n\nfrom perspective_ei.multispectral.competitors import *\n\nGDD_losses = [\n    dinv.loss.MCLoss(metric=LRMS_MSELoss(ratio)), \n    TVStructuralLoss(srf_from=\"average\")\n]\n\nSSQ_losses = [\n    SSQSpectralLoss(device=device), \n    SSQStructuralLoss(srf_from=\"average\"), \n    QNRLoss()\n]\n\nZPNN_losses = [\n    dinv.loss.MCLoss(metric=LRMS_L1Loss(ratio)), \n    ZPNN_CorrelationLoss(device=device)\n]\n\nPanGan_losses = [\n    dinv.loss.MCLoss(metric=LRMS_MSELoss(ratio)), \n    PanganStructuralLoss(srf_from=\"average\", device=device),\n    PanganAdvGenLoss(srf_from=\"average\", device=device),\n    PanganAdvDiscrimSpectralLoss(device=device),\n    PanganAdvDiscrimStructuralLoss(srf_from=\"average\", device=device)\n] # see perspective_ei.multispectral.competitors.pangan for training details"
  }
]